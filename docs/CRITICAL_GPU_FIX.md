# üî¥ CRITICAL: Performance Issue - Running on CPU Instead of GPU

## üêõ Root Cause Found

**The system is running on CPU, not GPU!**

### Diagnostic Results:
```
Detection (CPU): 226ms per frame  (Should be ~30-40ms on GPU)
Classification (CPU): 42ms per frame (Should be ~8-10ms on GPU)

Current FPS: 2.5 (CPU-bound)
Expected FPS with GPU: 12-15
```

**GPU is NOT available** - PyTorch is running in CPU-only mode.

---

## üîß Fix #1: Install PyTorch with CUDA Support (CRITICAL)

### Check Current PyTorch Installation:
```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

If it says `False`, you need to reinstall PyTorch with CUDA support.

### Uninstall Current PyTorch:
```bash
pip uninstall torch torchvision torchaudio -y
```

### Install PyTorch with CUDA 11.8:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Or CUDA 12.1 (if you have newer drivers):
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Verify Installation:
```bash
python check_gpu.py
```

Should show:
```
‚úì CUDA available: True
‚úì GPU 0: NVIDIA GeForce RTX...
GPU speedup: 5-7x
```

---

## üîß Fix #2: Reduce Logging Overhead

While not the main issue, logging can add 5-10% overhead.

### Current Issue:
- Every ROI collection logs a message
- Every classification logs a message
- File I/O for logs blocks briefly

### Fix: Reduce Logging Level in Production

**File: `src/utils/AppLogging.py`**

```python
# Change this line (line 28):
logger.setLevel(logging.DEBUG)  # ‚ùå Too verbose

# To:
logger.setLevel(logging.INFO)  # ‚úÖ Production mode
```

Or use environment variable:
```bash
export LOG_LEVEL=INFO
python main.py --source video.mp4
```

---

## üîß Fix #3: Disable Verbose Model Output

YOLO models can log verbose output even with `verbose=False`.

### Add to Detector/Classifier Initialization:

**File: `src/detection/UltralyticsDetector.py` (line 52)**
```python
self.model = YOLO(model_path)

# Add this to suppress YOLO logging:
import logging
logging.getLogger('ultralytics').setLevel(logging.WARNING)
```

**File: `src/classifier/UltralyticsClassifier.py` (line 50)**
```python
self.model = YOLO(model_path)

# Add this to suppress YOLO logging:
import logging
logging.getLogger('ultralytics').setLevel(logging.WARNING)
```

---

## üîß Fix #4: Optimize ROI Collection Logging

Currently logs every ROI collection - too verbose.

**File: `src/classifier/ROICollectorService.py` (line ~85)**

```python
# Change from:
logger.debug(
    f"[ROICollector] Track {self.track_id}: Collected ROI {self.collected_count}/{self.max_rois}, "
    f"quality={quality:.1f}"
)

# To: Only log every 5th ROI or when complete
if self.collected_count % 5 == 0 or self.collected_count == self.max_rois:
    logger.info(
        f"[ROICollector] Track {self.track_id}: {self.collected_count}/{self.max_rois} ROIs collected"
    )
```

---

## üîß Fix #5: Add Warmup for GPU Models

GPU models need warmup to reach peak performance.

**File: `src/detection/UltralyticsDetector.py`**

Add after model loading (line ~56):

```python
logger.info(f"[UltralyticsDetector] Model loaded, device: {self.device}")

# Warmup inference (for GPU)
if self.device == 'cuda':
    logger.info("[UltralyticsDetector] Warming up GPU...")
    dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)
    for _ in range(3):
        self.model(dummy_frame, device=self.device, verbose=False)
    logger.info("[UltralyticsDetector] GPU warmup complete")
```

**File: `src/classifier/UltralyticsClassifier.py`**

Add after model loading (line ~58):

```python
logger.info(f"[UltralyticsClassifier] Model loaded, device: {self.device}")

# Warmup inference (for GPU)
if self.device == 'cuda':
    logger.info("[UltralyticsClassifier] Warming up GPU...")
    dummy_roi = np.zeros((224, 224, 3), dtype=np.uint8)
    for _ in range(3):
        self.model(dummy_roi, device=self.device, verbose=False)
    logger.info("[UltralyticsClassifier] GPU warmup complete")
```

---

## üîß Fix #6: Optimize Frame Display

OpenCV's `imshow` can be slow if window is large.

**File: `src/app/ConveyorCounterApp.py` (line ~525)**

```python
# Before:
cv2.imshow("Conveyor Counter", annotated)

# After: Resize for faster display
if self.enable_display:
    display_frame = cv2.resize(annotated, (960, 540))  # 720p -> smaller
    cv2.imshow("Conveyor Counter", display_frame)
```

---

## üìä Expected Performance After Fixes

### With All Fixes Applied:

| Component | CPU Time | GPU Time | Improvement |
|-----------|----------|----------|-------------|
| Detection | 226ms | **35ms** | **6.5x faster** |
| Classification | 42ms | **8ms** | **5.3x faster** |
| Tracking | 10ms | 10ms | (same) |
| ROI Collection | 5ms | 5ms | (same) |
| Display | 10ms | 5ms | (optimized) |
| **Total** | **293ms** | **63ms** | **4.7x faster** |

### FPS:

| Mode | FPS | Status |
|------|-----|--------|
| Current (CPU) | 2.5 | ‚ùå Unacceptable |
| After GPU Fix | **15.8** | ‚úÖ **Excellent** |
| Target | 12-15 | ‚úÖ **Exceeded** |

---

## üöÄ Quick Start: Apply All Fixes

### 1. Install CUDA PyTorch (CRITICAL):
```bash
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. Verify GPU:
```bash
python check_gpu.py
```

Should see:
```
‚úì CUDA available: True
‚úì GPU speedup: 5-7x
```

### 3. Run System:
```bash
python main.py --source video.mp4
```

### 4. Check Logs:
Look for:
```
[UltralyticsDetector] Model loaded, device: cuda  ‚úÖ
[UltralyticsClassifier] Model loaded, device: cuda  ‚úÖ
```

If you see `device: cpu` ‚Üí GPU not detected!

---

## üîç Debugging GPU Issues

### Issue: CUDA Not Available

**Check NVIDIA Driver:**
```bash
nvidia-smi
```

Should show GPU info. If not, install NVIDIA drivers.

**Check CUDA Toolkit:**
```bash
nvcc --version
```

Should show CUDA version. Must match PyTorch CUDA version.

**Check PyTorch Build:**
```bash
python -c "import torch; print(torch.version.cuda)"
```

Should show CUDA version (e.g., `11.8`). If `None`, PyTorch is CPU-only.

### Issue: GPU Detected But Not Used

**Force GPU in Code:**

**File: `src/detection/DetectorFactory.py` (line ~53)**
```python
return UltralyticsDetector(
    model_path=model_path,
    confidence_threshold=confidence_threshold,
    device='cuda'  # Force GPU
)
```

**File: `src/classifier/ClassifierFactory.py` (line ~73)**
```python
return UltralyticsClassifier(
    model_path=model_path,
    classes=classes,
    device='cuda'  # Force GPU
)
```

---

## üéØ Priority Order

1. **CRITICAL: Install PyTorch with CUDA** ‚Üí 6x speedup
2. **HIGH: Add GPU warmup** ‚Üí Consistent performance
3. **MEDIUM: Reduce logging** ‚Üí 5-10% improvement
4. **LOW: Optimize display** ‚Üí 5ms saved

**Fix #1 alone will solve the 2.5 FPS problem!**

---

## ‚úÖ Verification Checklist

After applying fixes, verify:

- [ ] `python check_gpu.py` shows CUDA available
- [ ] Logs show `device: cuda` for detector and classifier
- [ ] FPS increases from 2.5 to 12-15
- [ ] GPU usage visible in `nvidia-smi` (should be ~50-80%)
- [ ] Classification worker processes jobs quickly

---

## üìù Additional Optimizations (If Still Needed)

If you're still not hitting 12-15 FPS after GPU fix:

### 1. Use Smaller Models
- Switch from YOLOv9 to YOLOv8n (nano)
- Faster but slightly less accurate

### 2. Reduce Input Resolution
```python
# In detector config
input_size = (416, 416)  # Instead of (640, 640)
```

### 3. Batch Classification
- Collect multiple ROIs
- Classify in batches for better GPU utilization

### 4. Use TensorRT (Advanced)
- Export models to TensorRT format
- 2-3x faster on NVIDIA GPUs
```bash
yolo export model=model.pt format=engine device=0
```

---

## üéâ Summary

**Root Cause:** Running on CPU instead of GPU  
**Main Fix:** Install PyTorch with CUDA support  
**Expected Result:** 2.5 FPS ‚Üí **15 FPS** (6x improvement)  
**Status:** Ready to implement!

Run:
```bash
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
python check_gpu.py
python main.py --source video.mp4
```

You should see **12-15 FPS immediately!** üöÄ
