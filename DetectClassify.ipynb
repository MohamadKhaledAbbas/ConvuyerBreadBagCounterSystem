{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T08:30:11.753107Z",
     "start_time": "2026-02-11T08:20:41.126925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Optional, Dict, Set\n",
    "from enum import Enum\n",
    "\n",
    "class FilterMode(Enum):\n",
    "    ALL = \"all\"\n",
    "    INCLUDE = \"include\"\n",
    "    EXCLUDE = \"exclude\"\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    model_detect_path: str = 'data/model/yolo_nano_detect_v12.pt.pt'\n",
    "    model_classify_path: str = 'data/model/yolo_small_classify_v15.pt.pt'\n",
    "    video_path: str = ''\n",
    "    output_base_dir: Optional[str] = None\n",
    "\n",
    "    # Detection settings\n",
    "    detection_confidence: float = 0.7\n",
    "    detection_filter_mode: FilterMode = FilterMode.ALL\n",
    "    detection_filter_classes: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Classification settings\n",
    "    classification_confidence: float = 0.5\n",
    "    classification_filter_mode: FilterMode = FilterMode.ALL\n",
    "    classification_filter_classes: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Processing settings\n",
    "    frame_skip: int = 100\n",
    "    min_crop_size: int = 5\n",
    "    save_format: str = \"jpg\"\n",
    "    save_quality: int = 95\n",
    "\n",
    "    # ‚≠ê NEW: Frame saving options\n",
    "    save_full_frames: bool = True           # Save the complete frame\n",
    "    save_crops: bool = True                  # Save cropped classifications\n",
    "    draw_boxes_on_frame: bool = False         # Draw bounding boxes on saved frames\n",
    "    box_thickness: int = 2                   # Bounding box line thickness\n",
    "\n",
    "    # Metadata\n",
    "    save_metadata: bool = True\n",
    "\n",
    "    # Debug settings\n",
    "    debug_mode: bool = False\n",
    "    debug_max_frames: int = 500\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.output_base_dir is None and self.video_path:\n",
    "            video_dir = os.path.dirname(self.video_path)\n",
    "            self.output_base_dir = os.path.join(video_dir, 'output')\n",
    "\n",
    "\n",
    "class DetectClassifyPipeline:\n",
    "    \"\"\"\n",
    "    Output Structure:\n",
    "    output/\n",
    "    ‚îî‚îÄ‚îÄ Detection/\n",
    "        ‚îî‚îÄ‚îÄ [detect_class_name]/\n",
    "            ‚îú‚îÄ‚îÄ Pure/                    # Full frames with this detection class\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ 0001.jpg\n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ 0002.jpg\n",
    "            ‚îî‚îÄ‚îÄ Classes/\n",
    "                ‚îî‚îÄ‚îÄ [classify_class_name]/  # Cropped & classified objects\n",
    "                    ‚îú‚îÄ‚îÄ 0001.jpg\n",
    "                    ‚îî‚îÄ‚îÄ 0002.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    # Colors for bounding boxes (BGR format)\n",
    "    COLORS = [\n",
    "        (0, 255, 0),    # Green\n",
    "        (255, 0, 0),    # Blue\n",
    "        (0, 0, 255),    # Red\n",
    "        (255, 255, 0),  # Cyan\n",
    "        (255, 0, 255),  # Magenta\n",
    "        (0, 255, 255),  # Yellow\n",
    "        (128, 0, 255),  # Purple\n",
    "        (255, 128, 0),  # Orange\n",
    "    ]\n",
    "\n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "\n",
    "        # Counters for sequential naming\n",
    "        # Structure: {detect_class: {classify_class: count}} for crops\n",
    "        # Structure: {detect_class: count} for pure frames\n",
    "        self.crop_counters: Dict[str, Dict[str, int]] = {}\n",
    "        self.frame_counters: Dict[str, int] = {}\n",
    "\n",
    "        # Track which frames we've already saved per detection class\n",
    "        # to avoid saving the same frame multiple times\n",
    "        self.saved_frames: Dict[str, Set[int]] = {}\n",
    "\n",
    "        self.metadata_log: List[Dict] = []\n",
    "\n",
    "        # Debug stats\n",
    "        self.debug_stats = {\n",
    "            'frames_processed': 0,\n",
    "            'total_detections': 0,\n",
    "            'filtered_by_detection_class': 0,\n",
    "            'filtered_by_crop_size': 0,\n",
    "            'filtered_by_classification_confidence': 0,\n",
    "            'filtered_by_classification_class': 0,\n",
    "            'frames_saved': 0,\n",
    "            'crops_saved': 0,\n",
    "            'save_failures': 0,\n",
    "        }\n",
    "\n",
    "        # Load models\n",
    "        print(\"üîß Loading YOLO models...\")\n",
    "        self.model_detect = YOLO(config.model_detect_path)\n",
    "        self.model_classify = YOLO(config.model_classify_path, task='classify')\n",
    "\n",
    "        self.detect_names = self.model_detect.names\n",
    "        self.classify_names = self.model_classify.names\n",
    "\n",
    "        # Get color mapping for detection classes\n",
    "        self.class_colors = {\n",
    "            name: self.COLORS[idx % len(self.COLORS)]\n",
    "            for idx, name in self.detect_names.items()\n",
    "        }\n",
    "\n",
    "        self._print_config()\n",
    "        self._setup_directories()\n",
    "\n",
    "    def _print_config(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã PIPELINE CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(\"\\nüì¶ Detection classes available:\")\n",
    "        for idx, name in self.detect_names.items():\n",
    "            print(f\"   {idx}: '{name}'\")\n",
    "\n",
    "        print(\"\\nüè∑Ô∏è  Classification classes available:\")\n",
    "        for idx, name in self.classify_names.items():\n",
    "            print(f\"   {idx}: '{name}'\")\n",
    "\n",
    "        print(f\"\\n‚öôÔ∏è  Filter Settings:\")\n",
    "        print(f\"   Detection mode: {self.config.detection_filter_mode.value}\")\n",
    "        print(f\"   Detection classes: {self.config.detection_filter_classes or 'ALL'}\")\n",
    "        print(f\"   Detection confidence: {self.config.detection_confidence}\")\n",
    "        print(f\"   Classification mode: {self.config.classification_filter_mode.value}\")\n",
    "        print(f\"   Classification classes: {self.config.classification_filter_classes or 'ALL'}\")\n",
    "        print(f\"   Classification confidence: {self.config.classification_confidence}\")\n",
    "\n",
    "        print(f\"\\nüíæ Save Settings:\")\n",
    "        print(f\"   Save full frames: {self.config.save_full_frames}\")\n",
    "        print(f\"   Save crops: {self.config.save_crops}\")\n",
    "        print(f\"   Draw boxes on frames: {self.config.draw_boxes_on_frame}\")\n",
    "\n",
    "        print(f\"\\nüìÅ Output Structure:\")\n",
    "        print(f\"   {self.config.output_base_dir}/\")\n",
    "        print(f\"   ‚îî‚îÄ‚îÄ Detection/\")\n",
    "        print(f\"       ‚îî‚îÄ‚îÄ [detect_class]/\")\n",
    "        print(f\"           ‚îú‚îÄ‚îÄ Pure/          # Full frames\")\n",
    "        print(f\"           ‚îî‚îÄ‚îÄ Classes/\")\n",
    "        print(f\"               ‚îî‚îÄ‚îÄ [classify_class]/  # Crops\")\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create output directory structure.\"\"\"\n",
    "        os.makedirs(self.config.output_base_dir, exist_ok=True)\n",
    "\n",
    "        # Test write access\n",
    "        test_file = os.path.join(self.config.output_base_dir, '_test.tmp')\n",
    "        try:\n",
    "            with open(test_file, 'w') as f:\n",
    "                f.write('test')\n",
    "            os.remove(test_file)\n",
    "            print(f\"\\n‚úÖ Output directory ready: {self.config.output_base_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå OUTPUT DIRECTORY NOT WRITABLE: {e}\")\n",
    "\n",
    "    def _get_pure_frame_path(self, detect_class: str) -> str:\n",
    "        \"\"\"Get path for saving full frame under detection class.\"\"\"\n",
    "        # Structure: output/Detection/[detect_class]/Pure/0001.jpg\n",
    "        pure_dir = os.path.join(\n",
    "            self.config.output_base_dir,\n",
    "            \"Detection\",\n",
    "            detect_class,\n",
    "            \"Pure\"\n",
    "        )\n",
    "        os.makedirs(pure_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize counter if needed\n",
    "        if detect_class not in self.frame_counters:\n",
    "            self.frame_counters[detect_class] = 1\n",
    "\n",
    "        base = os.path.splitext(os.path.basename(self.config.video_path))[0]\n",
    "        filename = f\"{base}_{self.frame_counters[detect_class]:04d}.{self.config.save_format}\"\n",
    "        self.frame_counters[detect_class] += 1\n",
    "\n",
    "        return os.path.join(pure_dir, filename)\n",
    "\n",
    "    def _get_crop_path(self, detect_class: str, classify_class: str) -> str:\n",
    "        \"\"\"Get path for saving cropped classification.\"\"\"\n",
    "        # Structure: output/Detection/[detect_class]/Classes/[classify_class]/0001.jpg\n",
    "        crop_dir = os.path.join(\n",
    "            self.config.output_base_dir,\n",
    "            \"Detection\",\n",
    "            detect_class,\n",
    "            \"Classes\",\n",
    "            classify_class\n",
    "        )\n",
    "        os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize counters if needed\n",
    "        if detect_class not in self.crop_counters:\n",
    "            self.crop_counters[detect_class] = {}\n",
    "        if classify_class not in self.crop_counters[detect_class]:\n",
    "            self.crop_counters[detect_class][classify_class] = 1\n",
    "\n",
    "        filename = f\"{self.crop_counters[detect_class][classify_class]:04d}.{self.config.save_format}\"\n",
    "        self.crop_counters[detect_class][classify_class] += 1\n",
    "\n",
    "        return os.path.join(crop_dir, filename)\n",
    "\n",
    "    def _should_process_detection(self, class_name: str) -> bool:\n",
    "        mode = self.config.detection_filter_mode\n",
    "        filter_classes = self.config.detection_filter_classes\n",
    "\n",
    "        if mode == FilterMode.ALL:\n",
    "            return True\n",
    "        elif mode == FilterMode.INCLUDE:\n",
    "            return class_name in filter_classes\n",
    "        elif mode == FilterMode.EXCLUDE:\n",
    "            return class_name not in filter_classes\n",
    "        return True\n",
    "\n",
    "    def _should_save_classification(self, class_name: str, confidence: float) -> tuple:\n",
    "        \"\"\"Returns (should_save, reason_if_not)\"\"\"\n",
    "        if confidence < self.config.classification_confidence:\n",
    "            return False, f\"confidence {confidence:.3f} < {self.config.classification_confidence}\"\n",
    "\n",
    "        mode = self.config.classification_filter_mode\n",
    "        filter_classes = self.config.classification_filter_classes\n",
    "\n",
    "        if mode == FilterMode.ALL:\n",
    "            return True, \"\"\n",
    "        elif mode == FilterMode.INCLUDE:\n",
    "            if class_name in filter_classes:\n",
    "                return True, \"\"\n",
    "            return False, f\"'{class_name}' not in {filter_classes}\"\n",
    "        elif mode == FilterMode.EXCLUDE:\n",
    "            if class_name not in filter_classes:\n",
    "                return True, \"\"\n",
    "            return False, f\"'{class_name}' is excluded\"\n",
    "        return True, \"\"\n",
    "\n",
    "    def _save_image(self, path: str, image) -> bool:\n",
    "        \"\"\"Save image with error handling.\"\"\"\n",
    "        try:\n",
    "            if self.config.save_format == \"jpg\":\n",
    "                success = cv2.imwrite(path, image,\n",
    "                    [cv2.IMWRITE_JPEG_QUALITY, self.config.save_quality])\n",
    "            else:\n",
    "                success = cv2.imwrite(path, image)\n",
    "\n",
    "            if not success:\n",
    "                print(f\"‚ùå cv2.imwrite failed: {path}\")\n",
    "                self.debug_stats['save_failures'] += 1\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Save error: {e}\")\n",
    "            self.debug_stats['save_failures'] += 1\n",
    "            return False\n",
    "\n",
    "    def _draw_boxes_on_frame(self, frame, detections_info: List[Dict]):\n",
    "        \"\"\"Draw bounding boxes and labels on frame.\"\"\"\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        for det in detections_info:\n",
    "            x1, y1, x2, y2 = det['bbox']\n",
    "            detect_class = det['detect_class']\n",
    "            classify_class = det['classify_class']\n",
    "            classify_conf = det['classify_conf']\n",
    "\n",
    "            color = self.class_colors.get(detect_class, (0, 255, 0))\n",
    "\n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, self.config.box_thickness)\n",
    "\n",
    "            # Create label\n",
    "            label = f\"{classify_class} ({classify_conf:.2f})\"\n",
    "\n",
    "            # Get label size for background\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.5\n",
    "            thickness = 1\n",
    "            (label_w, label_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "\n",
    "            # Draw label background\n",
    "            cv2.rectangle(frame_copy,\n",
    "                         (x1, y1 - label_h - 10),\n",
    "                         (x1 + label_w + 5, y1),\n",
    "                         color, -1)\n",
    "\n",
    "            # Draw label text\n",
    "            cv2.putText(frame_copy, label,\n",
    "                       (x1 + 2, y1 - 5),\n",
    "                       font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "        return frame_copy\n",
    "\n",
    "    def process_frame(self, frame, frame_number: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Process a single frame.\n",
    "        Returns dict with counts: {'frames_saved': n, 'crops_saved': m}\n",
    "        \"\"\"\n",
    "        if frame is None:\n",
    "            return {'frames_saved': 0, 'crops_saved': 0}\n",
    "\n",
    "        self.debug_stats['frames_processed'] += 1\n",
    "        results = {'frames_saved': 0, 'crops_saved': 0}\n",
    "\n",
    "        # Run detection\n",
    "        results_detect = self.model_detect(\n",
    "            frame,\n",
    "            conf=self.config.detection_confidence,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not results_detect or not results_detect[0].boxes:\n",
    "            return results\n",
    "\n",
    "        detections = results_detect[0].boxes\n",
    "        self.debug_stats['total_detections'] += len(detections)\n",
    "\n",
    "        # Group detections by detection class\n",
    "        # Structure: {detect_class: [list of detection info]}\n",
    "        detections_by_class: Dict[str, List[Dict]] = {}\n",
    "\n",
    "        if self.config.debug_mode:\n",
    "            print(f\"\\nüé¨ Frame {frame_number}: {len(detections)} detections\")\n",
    "\n",
    "        for idx, box in enumerate(detections):\n",
    "            detect_class_id = int(box.cls[0])\n",
    "            detect_class_name = self.detect_names[detect_class_id]\n",
    "            detect_confidence = float(box.conf[0])\n",
    "\n",
    "            if self.config.debug_mode:\n",
    "                print(f\"   [{idx}] Detected: '{detect_class_name}' (conf: {detect_confidence:.3f})\")\n",
    "\n",
    "            # Filter by detection class\n",
    "            if not self._should_process_detection(detect_class_name):\n",
    "                self.debug_stats['filtered_by_detection_class'] += 1\n",
    "                if self.config.debug_mode:\n",
    "                    print(f\"      ‚è≠Ô∏è  Skipped: detection class filter\")\n",
    "                continue\n",
    "\n",
    "            # Get bounding box and crop\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "            crop_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Validate crop size\n",
    "            if (crop_img.size == 0 or\n",
    "                crop_img.shape[0] < self.config.min_crop_size or\n",
    "                crop_img.shape[1] < self.config.min_crop_size):\n",
    "                self.debug_stats['filtered_by_crop_size'] += 1\n",
    "                if self.config.debug_mode:\n",
    "                    print(f\"      ‚è≠Ô∏è  Skipped: crop too small\")\n",
    "                continue\n",
    "\n",
    "            # Run classification\n",
    "            results_classify = self.model_classify(crop_img, verbose=False)\n",
    "\n",
    "            classify_class_id = results_classify[0].probs.top1\n",
    "            classify_class_name = self.classify_names[classify_class_id]\n",
    "            classify_confidence = float(results_classify[0].probs.top1conf)\n",
    "\n",
    "            if self.config.debug_mode:\n",
    "                print(f\"      üè∑Ô∏è  Classified: '{classify_class_name}' (conf: {classify_confidence:.3f})\")\n",
    "\n",
    "            # Filter by classification\n",
    "            should_save, reason = self._should_save_classification(classify_class_name, classify_confidence)\n",
    "            if not should_save:\n",
    "                if classify_confidence < self.config.classification_confidence:\n",
    "                    self.debug_stats['filtered_by_classification_confidence'] += 1\n",
    "                else:\n",
    "                    self.debug_stats['filtered_by_classification_class'] += 1\n",
    "                if self.config.debug_mode:\n",
    "                    print(f\"      ‚è≠Ô∏è  Skipped: {reason}\")\n",
    "                continue\n",
    "\n",
    "            # Store detection info\n",
    "            det_info = {\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'detect_class': detect_class_name,\n",
    "                'detect_conf': detect_confidence,\n",
    "                'classify_class': classify_class_name,\n",
    "                'classify_conf': classify_confidence,\n",
    "                'crop_img': crop_img,\n",
    "            }\n",
    "\n",
    "            if detect_class_name not in detections_by_class:\n",
    "                detections_by_class[detect_class_name] = []\n",
    "            detections_by_class[detect_class_name].append(det_info)\n",
    "\n",
    "        # Now save frames and crops organized by detection class\n",
    "        for detect_class, det_list in detections_by_class.items():\n",
    "\n",
    "            # Initialize saved frames tracking\n",
    "            if detect_class not in self.saved_frames:\n",
    "                self.saved_frames[detect_class] = set()\n",
    "\n",
    "            # Save full frame (once per detection class per frame)\n",
    "            if self.config.save_full_frames:\n",
    "                if frame_number not in self.saved_frames[detect_class]:\n",
    "\n",
    "                    if self.config.draw_boxes_on_frame:\n",
    "                        frame_to_save = self._draw_boxes_on_frame(frame, det_list)\n",
    "                    else:\n",
    "                        frame_to_save = frame\n",
    "\n",
    "                    frame_path = self._get_pure_frame_path(detect_class)\n",
    "\n",
    "                    if self._save_image(frame_path, frame_to_save):\n",
    "                        results['frames_saved'] += 1\n",
    "                        self.debug_stats['frames_saved'] += 1\n",
    "                        self.saved_frames[detect_class].add(frame_number)\n",
    "\n",
    "                        if self.config.debug_mode:\n",
    "                            print(f\"      üíæ Saved frame: {frame_path}\")\n",
    "\n",
    "            # Save crops\n",
    "            if self.config.save_crops:\n",
    "                for det_info in det_list:\n",
    "                    crop_path = self._get_crop_path(\n",
    "                        detect_class,\n",
    "                        det_info['classify_class']\n",
    "                    )\n",
    "\n",
    "                    if self._save_image(crop_path, det_info['crop_img']):\n",
    "                        results['crops_saved'] += 1\n",
    "                        self.debug_stats['crops_saved'] += 1\n",
    "\n",
    "                        if self.config.debug_mode:\n",
    "                            print(f\"      üíæ Saved crop: {crop_path}\")\n",
    "\n",
    "                    # Log metadata\n",
    "                    if self.config.save_metadata:\n",
    "                        self.metadata_log.append({\n",
    "                            'frame': frame_number,\n",
    "                            'crop_file': crop_path,\n",
    "                            'detection_class': det_info['detect_class'],\n",
    "                            'detection_confidence': round(det_info['detect_conf'], 4),\n",
    "                            'classification_class': det_info['classify_class'],\n",
    "                            'classification_confidence': round(det_info['classify_conf'], 4),\n",
    "                            'bbox': det_info['bbox']\n",
    "                        })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def process_video(self):\n",
    "        \"\"\"Process the entire video.\"\"\"\n",
    "        if not os.path.exists(self.config.video_path):\n",
    "            print(f\"‚ùå ERROR: Video not found: {self.config.video_path}\")\n",
    "            return\n",
    "\n",
    "        cap = cv2.VideoCapture(self.config.video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"‚ùå ERROR: Could not open video: {self.config.video_path}\")\n",
    "            return\n",
    "\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        print(f\"\\nüé• Video: {os.path.basename(self.config.video_path)}\")\n",
    "        print(f\"   Frames: {frame_count} | FPS: {fps:.2f}\")\n",
    "        print(f\"   Processing every {self.config.frame_skip} frames\")\n",
    "        print(f\"   Estimated frames to process: {frame_count // self.config.frame_skip}\")\n",
    "\n",
    "        if self.config.debug_mode:\n",
    "            print(f\"   üêõ DEBUG MODE: Stopping after {self.config.debug_max_frames} frames\")\n",
    "\n",
    "        total_frames_saved = 0\n",
    "        total_crops_saved = 0\n",
    "        frame_number = 0\n",
    "\n",
    "        max_frames = self.config.debug_max_frames if self.config.debug_mode else frame_count\n",
    "\n",
    "        with tqdm(total=min(frame_count, max_frames), desc=\"Processing\") as pbar:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame_number += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if self.config.debug_mode and frame_number > self.config.debug_max_frames:\n",
    "                    print(f\"\\nüõë Debug mode: Stopping after {self.config.debug_max_frames} frames\")\n",
    "                    break\n",
    "\n",
    "                if frame_number % self.config.frame_skip != 0:\n",
    "                    continue\n",
    "\n",
    "                result = self.process_frame(frame, frame_number)\n",
    "                total_frames_saved += result['frames_saved']\n",
    "                total_crops_saved += result['crops_saved']\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'frames': total_frames_saved,\n",
    "                    'crops': total_crops_saved\n",
    "                })\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Save metadata\n",
    "        if self.config.save_metadata and self.metadata_log:\n",
    "            metadata_path = os.path.join(self.config.output_base_dir, 'metadata.json')\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump({\n",
    "                    'config': asdict(self.config),\n",
    "                    'crops': self.metadata_log,\n",
    "                    'frame_counts': self.frame_counters,\n",
    "                    'crop_counts': self.crop_counters,\n",
    "                    'debug_stats': self.debug_stats,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }, f, indent=2, default=str)\n",
    "            print(f\"\\nüìÑ Metadata saved: {metadata_path}\")\n",
    "\n",
    "        self._print_summary()\n",
    "\n",
    "    def _print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä PROCESSING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        stats = self.debug_stats\n",
    "\n",
    "        print(f\"\\nüìà Statistics:\")\n",
    "        print(f\"   Frames processed:              {stats['frames_processed']}\")\n",
    "        print(f\"   Total detections:              {stats['total_detections']}\")\n",
    "        print(f\"   Filtered (detection class):    {stats['filtered_by_detection_class']}\")\n",
    "        print(f\"   Filtered (crop size):          {stats['filtered_by_crop_size']}\")\n",
    "        print(f\"   Filtered (classify conf):      {stats['filtered_by_classification_confidence']}\")\n",
    "        print(f\"   Filtered (classify class):     {stats['filtered_by_classification_class']}\")\n",
    "\n",
    "        print(f\"\\nüíæ Saved:\")\n",
    "        print(f\"   Full frames saved:             {stats['frames_saved']} ‚úÖ\")\n",
    "        print(f\"   Crops saved:                   {stats['crops_saved']} ‚úÖ\")\n",
    "        print(f\"   Save failures:                 {stats['save_failures']} ‚ùå\")\n",
    "\n",
    "        # Print breakdown by class\n",
    "        if self.frame_counters:\n",
    "            print(f\"\\nüìÅ Frames per detection class:\")\n",
    "            for detect_class, count in self.frame_counters.items():\n",
    "                print(f\"   {detect_class}: {count - 1} frames\")\n",
    "\n",
    "        if self.crop_counters:\n",
    "            print(f\"\\nüìÅ Crops breakdown:\")\n",
    "            for detect_class, classify_counts in self.crop_counters.items():\n",
    "                print(f\"   {detect_class}/\")\n",
    "                for classify_class, count in classify_counts.items():\n",
    "                    print(f\"      ‚îî‚îÄ‚îÄ {classify_class}: {count - 1} crops\")\n",
    "\n",
    "        print(f\"\\nüìÇ Output folder: {self.config.output_base_dir}\")\n",
    "\n",
    "        # Show directory structure\n",
    "        print(\"\\nüìÇ Directory structure created:\")\n",
    "        self._print_directory_tree()\n",
    "\n",
    "    def _print_directory_tree(self, max_files: int = 3):\n",
    "        \"\"\"Print the output directory structure.\"\"\"\n",
    "        for root, dirs, files in os.walk(self.config.output_base_dir):\n",
    "            level = root.replace(self.config.output_base_dir, '').count(os.sep)\n",
    "            indent = '   ' * level\n",
    "            folder_name = os.path.basename(root)\n",
    "            file_count = len(files)\n",
    "\n",
    "            if file_count > 0:\n",
    "                print(f\"{indent}üìÅ {folder_name}/ ({file_count} files)\")\n",
    "            else:\n",
    "                print(f\"{indent}üìÅ {folder_name}/\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EXAMPLE CONFIGURATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration 1: Save everything (both frame types, all classes)\n",
    "config_all = PipelineConfig(\n",
    "    video_path='D:\\\\Recordings\\\\New_Recordings\\\\GREEN.mp4',\n",
    "    detection_filter_mode=FilterMode.ALL,\n",
    "    classification_filter_mode=FilterMode.ALL,\n",
    "    detection_confidence=0.7,\n",
    "    classification_confidence=0.7,\n",
    "    save_full_frames=True,\n",
    "    save_crops=True,\n",
    "    draw_boxes_on_frame=True,\n",
    "    frame_skip=60,\n",
    ")\n",
    "\n",
    "# Configuration 3: Both bag types, specific colors only\n",
    "config_specific_colors = PipelineConfig(\n",
    "    video_path='D:\\\\Recordings\\\\New_Recordings\\\\GREEN.mp4',\n",
    "    detection_filter_mode=FilterMode.INCLUDE,\n",
    "    detection_filter_classes=['bread-bag-opened', 'bread-bag-closed'],\n",
    "    classification_filter_mode=FilterMode.INCLUDE,\n",
    "    classification_filter_classes=['Red_Yellow', 'Blue_Yellow', 'Green_Yellow'],\n",
    "    detection_confidence=0.7,\n",
    "    classification_confidence=0.8,\n",
    "    save_full_frames=True,\n",
    "    save_crops=True,\n",
    "    draw_boxes_on_frame=True,\n",
    "    frame_skip=60,\n",
    ")\n",
    "\n",
    "# Configuration 4: Only crops, no frames (faster, less disk space)\n",
    "config_crops_only = PipelineConfig(\n",
    "    video_path='D:\\\\Recordings\\\\New_Recordings\\\\GREEN.mp4',\n",
    "    detection_filter_mode=FilterMode.ALL,\n",
    "    classification_filter_mode=FilterMode.ALL,\n",
    "    detection_confidence=0.7,\n",
    "    classification_confidence=0.8,\n",
    "    save_full_frames=False,  # Don't save full frames\n",
    "    save_crops=True,\n",
    "    frame_skip=60,\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "# Choose your configuration\n",
    "config = PipelineConfig(\n",
    "    video_path='D:\\\\Recordings\\\\New_Recordings\\\\GREEN.mp4',\n",
    "\n",
    "    # Detection settings\n",
    "    detection_filter_mode=FilterMode.INCLUDE,\n",
    "    detection_filter_classes=['bread-bag-closed'],  # Both types\n",
    "    detection_confidence=0.7,\n",
    "\n",
    "    # Classification settings\n",
    "    classification_filter_mode=FilterMode.ALL,  # All color types\n",
    "    classification_confidence=0.8,\n",
    "\n",
    "    # Save settings\n",
    "    save_full_frames=True,\n",
    "    save_crops=True,\n",
    "    draw_boxes_on_frame=True,\n",
    "\n",
    "    # Processing\n",
    "    frame_skip=10,\n",
    "\n",
    "    # Debug (set to False for full run)\n",
    "    debug_mode=True,\n",
    "    debug_max_frames=20000,\n",
    ")\n",
    "\n",
    "# Configuration 2: Only opened bags\n",
    "\n",
    "config_opened = PipelineConfig(\n",
    "    video_path='D:\\\\Recordings\\\\2026_02_05\\\\2026_02_11\\\\mp4\\\\output_2026-02-11_02-36-34.mp4',\n",
    "    detection_filter_mode=FilterMode.INCLUDE,\n",
    "    detection_filter_classes=['bread-bag'],\n",
    "    detection_confidence=0.75,\n",
    "    classification_confidence=0.75,\n",
    "    save_full_frames=True,\n",
    "    save_crops=True,\n",
    "    draw_boxes_on_frame=False,\n",
    "    frame_skip=14,\n",
    "    debug_mode=False,\n",
    "    debug_max_frames=10000\n",
    ")\n",
    "\n",
    "pipeline = DetectClassifyPipeline(config_opened)\n",
    "pipeline.process_video()\n",
    "# Notebook helper: process all .mp4 files in a directory with your existing pipeline.\n",
    "# Paste this into a notebook cell (assumes config_opened, PipelineConfig, DetectClassifyPipeline are already defined\n",
    "# in the notebook or are importable)."
   ],
   "id": "a17b772bbfd5f303",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading YOLO models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001D67AFA80E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\0001_MyFiles\\0016_Projects\\0002_ProjectBased\\0012_ConvuyerBreadBagCounterSystem\\.venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\0001_MyFiles\\0016_Projects\\0002_ProjectBased\\0012_ConvuyerBreadBagCounterSystem\\.venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 282, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã PIPELINE CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "üì¶ Detection classes available:\n",
      "   0: 'bread-bag'\n",
      "\n",
      "üè∑Ô∏è  Classification classes available:\n",
      "   0: 'Blue_Yellow'\n",
      "   1: 'Bran'\n",
      "   2: 'Brown_Orange_Family'\n",
      "   3: 'Green_Yellow'\n",
      "   4: 'Red_Yellow'\n",
      "   5: 'Rejected'\n",
      "   6: 'Wheatberry'\n",
      "\n",
      "‚öôÔ∏è  Filter Settings:\n",
      "   Detection mode: include\n",
      "   Detection classes: ['bread-bag']\n",
      "   Detection confidence: 0.75\n",
      "   Classification mode: all\n",
      "   Classification classes: ALL\n",
      "   Classification confidence: 0.75\n",
      "\n",
      "üíæ Save Settings:\n",
      "   Save full frames: True\n",
      "   Save crops: True\n",
      "   Draw boxes on frames: False\n",
      "\n",
      "üìÅ Output Structure:\n",
      "   D:\\Recordings\\2026_02_05\\2026_02_11\\mp4\\output/\n",
      "   ‚îî‚îÄ‚îÄ Detection/\n",
      "       ‚îî‚îÄ‚îÄ [detect_class]/\n",
      "           ‚îú‚îÄ‚îÄ Pure/          # Full frames\n",
      "           ‚îî‚îÄ‚îÄ Classes/\n",
      "               ‚îî‚îÄ‚îÄ [classify_class]/  # Crops\n",
      "\n",
      "‚úÖ Output directory ready: D:\\Recordings\\2026_02_05\\2026_02_11\\mp4\\output\n",
      "\n",
      "üé• Video: output_2026-02-11_02-36-34.mp4\n",
      "   Frames: 25200 | FPS: 14.00\n",
      "   Processing every 14 frames\n",
      "   Estimated frames to process: 1800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing:   0%|          | 0/25200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5ce13ede12347dfbf0aebe8e994f52a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Metadata saved: D:\\Recordings\\2026_02_05\\2026_02_11\\mp4\\output\\metadata.json\n",
      "\n",
      "============================================================\n",
      "üìä PROCESSING SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìà Statistics:\n",
      "   Frames processed:              1800\n",
      "   Total detections:              2081\n",
      "   Filtered (detection class):    0\n",
      "   Filtered (crop size):          0\n",
      "   Filtered (classify conf):      356\n",
      "   Filtered (classify class):     0\n",
      "\n",
      "üíæ Saved:\n",
      "   Full frames saved:             1258 ‚úÖ\n",
      "   Crops saved:                   1725 ‚úÖ\n",
      "   Save failures:                 0 ‚ùå\n",
      "\n",
      "üìÅ Frames per detection class:\n",
      "   bread-bag: 1258 frames\n",
      "\n",
      "üìÅ Crops breakdown:\n",
      "   bread-bag/\n",
      "      ‚îî‚îÄ‚îÄ Brown_Orange_Family: 1341 crops\n",
      "      ‚îî‚îÄ‚îÄ Rejected: 99 crops\n",
      "      ‚îî‚îÄ‚îÄ Blue_Yellow: 102 crops\n",
      "      ‚îî‚îÄ‚îÄ Bran: 21 crops\n",
      "      ‚îî‚îÄ‚îÄ Red_Yellow: 1 crops\n",
      "      ‚îî‚îÄ‚îÄ Wheatberry: 18 crops\n",
      "      ‚îî‚îÄ‚îÄ Green_Yellow: 143 crops\n",
      "\n",
      "üìÇ Output folder: D:\\Recordings\\2026_02_05\\2026_02_11\\mp4\\output\n",
      "\n",
      "üìÇ Directory structure created:\n",
      "üìÅ output/ (1 files)\n",
      "   üìÅ Detection/\n",
      "      üìÅ bread-bag/\n",
      "         üìÅ Classes/\n",
      "            üìÅ Blue_Yellow/ (102 files)\n",
      "            üìÅ Bran/ (21 files)\n",
      "            üìÅ Brown_Orange_Family/ (1154 files)\n",
      "            üìÅ Green_Yellow/ (143 files)\n",
      "            üìÅ Red_Yellow/ (1 files)\n",
      "            üìÅ Rejected/ (77 files)\n",
      "            üìÅ Wheatberry/ (18 files)\n",
      "         üìÅ Pure/ (1258 files)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: nice progress bar if tqdm is installed; falls back to a simple iterator.\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    try:\n",
    "        from tqdm import tqdm  # terminal tqdm\n",
    "    except Exception:\n",
    "        tqdm = lambda x, **kw: x  # fallback: identity\n",
    "\n",
    "def process_all_mp4s(\n",
    "    mp4_dir=\"D:\\\\Recordings\\\\2026_02_05\\\\2026_02_09\\\\mp4\",\n",
    "    recursive=False,\n",
    "    dry_run=False,\n",
    "    stop_on_error=False,\n",
    "    show_progress=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process every .mp4 in `mp4_dir` using your existing `config_opened` and\n",
    "    `DetectClassifyPipeline`.\n",
    "\n",
    "    Parameters\n",
    "    - mp4_dir: directory to search for .mp4 files (default \"mp4\")\n",
    "    - recursive: whether to search recursively (default False)\n",
    "    - dry_run: if True, only lists files found (no processing)\n",
    "    - stop_on_error: if True, stops on first exception\n",
    "    - show_progress: use tqdm progress bar when available\n",
    "    \"\"\"\n",
    "    # Ensure the base config/pipeline exist in notebook namespace or import them\n",
    "    if \"config_opened\" not in globals():\n",
    "        raise RuntimeError(\"config_opened not found in notebook globals. Define it or import it first.\")\n",
    "    if \"DetectClassifyPipeline\" not in globals():\n",
    "        raise RuntimeError(\"DetectClassifyPipeline not found in notebook globals. Define it or import it first.\")\n",
    "\n",
    "    mp4_dir = Path(mp4_dir)\n",
    "    if not mp4_dir.exists() or not mp4_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"MP4 directory not found: {mp4_dir}\")\n",
    "\n",
    "    files = sorted(mp4_dir.rglob(\"*.mp4\") if recursive else mp4_dir.glob(\"*.mp4\"))\n",
    "    files = list(files)\n",
    "    if not files:\n",
    "        print(f\"No .mp4 files found in {mp4_dir} (recursive={recursive}).\")\n",
    "        return\n",
    "\n",
    "    success = 0\n",
    "    failed = 0\n",
    "\n",
    "    iterator = tqdm(files) if show_progress else files\n",
    "    for mp4_path in iterator:\n",
    "        print(f\"\\n--- Processing: {mp4_path} ---\")\n",
    "        if dry_run:\n",
    "            continue\n",
    "        try:\n",
    "            cfg = copy.deepcopy(config_opened)\n",
    "            cfg.video_path = str(mp4_path)  # update only video path\n",
    "            pipeline = DetectClassifyPipeline(cfg)\n",
    "            pipeline.process_video()\n",
    "            success += 1\n",
    "        except Exception:\n",
    "            failed += 1\n",
    "            print(f\"Error processing {mp4_path}:\")\n",
    "            traceback.print_exc()\n",
    "            if stop_on_error:\n",
    "                break\n",
    "\n",
    "    print(f\"\\nFinished. Processed: {success + failed}, Success: {success}, Failed: {failed}\")\n",
    "\n",
    "# Example usage in notebook cells:\n",
    "process_all_mp4s()                  # process files in ./mp4 (non-recursive)\n",
    "# process_all_mp4s(mp4_dir=\"mp4\", recursive=True)  # recursive\n",
    "# process_all_mp4s(dry_run=True)      # list files only"
   ],
   "id": "c0830f53c02fd62e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
